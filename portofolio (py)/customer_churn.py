# -*- coding: utf-8 -*-
"""Sertifikasi BNSP - Dheva Fauzia Chema.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LXcouXl3K5XIQH4qpK_KfT5eJavSLXHT

### **Mengumpulkan Data**
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import scipy.stats as stats
import seaborn as sns

data = pd.read_csv('/content/drive/MyDrive/sertifikasi/Bank_Churn_Modelling.csv')
data

"""### **Menelaah Data**"""

# Analisis Tipe Data
data.info()

# Analisis Karakteristik Data
data.describe()

# Distribusi Data
data.hist(bins=10,figsize=(15,10))
plt.show()

data.corr()

correlation_matrix = data.corr()

# Membuat heatmap
plt.figure(figsize=(7, 7))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=.5)
plt.title('Heatmap Korelasi Antar Kolom')
plt.show()

"""### **Memvalidasi Data**"""

print(data.isnull().sum())

data.nunique()

"""### **Menentukan Objek Data**"""

jumlah_row, jumlah_column = data.shape
print(f'Jumlah Baris: {jumlah_row}')
print(f'Jumlah Kolom: {jumlah_column}')

"""### **Membersihkan Data**"""

# Memeriksa Konsisten Data
print(data['CustomerId'].unique())
print(data['Surname'].unique())
print(data['CreditScore'].unique())
print(data['Geography'].unique())
print(data['Gender'].unique())
print(data['Age'].unique())
print(data['Tenure'].unique())
print(data['Balance'].unique())
print(data['NumOfProducts'].unique())
print(data['HasCrCard'].unique())
print(data['IsActiveMember'].unique())
print(data['EstimatedSalary'].unique())
print(data['Exited'].unique())

data['Gender'] = data['Gender'].str.replace('Female', 'F')

data['Gender'].unique()

data['Gender'] = data['Gender'].str.replace('Male', 'M')

data['Gender'].unique()

# Mencari baris mana yang memiliki nilai NaN pada kolom 'Gender'
rows_with_nan = data[data['Gender'].isnull()]

print("Baris dengan nilai NaN pada kolom 'Gender':")
print(rows_with_nan)

# Menghitung nilai mode dari kolom 'Gender' tanpa memperhitungkan nilai NaN
gender_mode = data['Gender'].dropna().mode()[0]

# Mengisi nilai NaN dengan nilai mode
data['Gender'].fillna(gender_mode, inplace=True)

# Mengindentifikasi Nilai NaN pada kolom "Gender"
data[data['RowNumber'] == 577]

data['Gender'].unique()

# Mencari baris mana yang memiliki nilai NaN pada kolom 'Age'
rows_with_nan = data[data['Age'].isnull()]

print("Baris dengan nilai NaN pada kolom 'Age':")
print(rows_with_nan)

# Menghitung nilai rata-rata dari kolom 'Age' tanpa memperhitungkan nilai NaN
age_mean = data['Age'].dropna().mode()[0]

# Mengisi nilai NaN dengan nilai rata-rata ('age_mean')
data['Age'].fillna(age_mean, inplace=True)

# Mengindentifikasi Nilai NaN pada kolom "Age"
data[data['RowNumber'] == 1831]

data['Age'].unique()

# Mengidentifikasi Missing Value
data.isnull().sum()

data.info()

# Data Numerik
numeric_data = data.select_dtypes(include=np.number)
numeric_data

"""Menentukan nilai Z-Score pada tiap barisnya untuk melihat seberapa jauh nilai data dari mean dalam satuan standar deviasi."""

# Score Z-Score
z = np.abs(stats.zscore(numeric_data))
print(z)

# Nilai yang termasuk Outliers
threshold = 3
print(np.where(z > 3))

"""Data outliers diatas merupakan data yang nilainya diluar dari rentang 3."""

# Data Sesudah Outliers Dihapus
data_clean = data[(z < 3).all(axis=1)]
data_clean

# Feature Engineering
target_correlation = data_clean.corr()['Exited'].sort_values(ascending=False)
target_correlation

# Pemisahan kolom yang tidak mendekati correlation target
threshold = 0.01
columns = target_correlation[abs(target_correlation)<=threshold].index
df_clean = data_clean.drop(columns=columns)
df_clean

df_clean.info()

df_clean

df_clean.hist(bins=10,figsize=(10,10))
plt.show()

# Membuat histogram untuk melihat kategori gender yang meninggalkan bank
plt.figure(figsize=(8, 5))

# Menghitung jumlah pengguna yang meninggalkan dan tidak meninggalkan berdasarkan jenis kelamin
exit_counts = df_clean[df_clean['Exited'] == 1]['Gender'].value_counts()
not_exit_counts = df_clean[df_clean['Exited'] == 0]['Gender'].value_counts()

# Menentukan posisi bar untuk setiap jenis kelamin
bar_width = 0.35
bar_positions_exit = range(len(exit_counts))
bar_positions_not_exit = [pos + bar_width for pos in bar_positions_exit]

# Menampilkan bar chart
plt.bar(bar_positions_exit, exit_counts, width=bar_width, color='salmon', label='Meninggalkan')
plt.bar(bar_positions_not_exit, not_exit_counts, width=bar_width, color='skyblue', label='Tidak Meninggalkan')

# Menambahkan label dan judul
plt.xlabel('Gender')
plt.ylabel('Jumlah')
plt.title('Jumlah Pengguna yang Meninggalkan dan Tidak Meninggalkan Bank Berdasarkan Gender')
plt.xticks([pos + bar_width / 2 for pos in bar_positions_exit], exit_counts.index)
plt.legend(title='Status Keluar')

# Menampilkan grafik
plt.show()

"""### **Mengkonstruksi Data**"""

# Drop kolom yang tidak diperlukan saat pemodelan
kolom = ['RowNumber', 'Surname', 'Geography']
df_clean = df_clean.drop(kolom, axis=1)

df_clean.info()

# Mengubah tipe data 'Age' dari float menjadi int
df_clean['Age'] = df_clean['Age'].astype(int)
df_clean.info()

"""### **Menentukan Label Data**"""

from sklearn.preprocessing import LabelEncoder

# Mengubah variabel kategorikal 'Gender' menjadi numerik dalam bentuk label
label_encoder = LabelEncoder()
df_clean['Gender'] = label_encoder.fit_transform(df_clean['Gender'])

df_clean

# Memisahkan target dari dataframe
X = df_clean.drop('Exited', axis=1)
y = df_clean['Exited']

X

y

"""### **Membangun Model**"""

from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
import xgboost as xgb
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score

# Transformasi tabel
data_scaler = MinMaxScaler()
X_scaled = data_scaler.fit_transform(X)
X = pd.DataFrame(X_scaled, columns=X.columns)

# Pemisahan data training dan data test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

# K-Nearest Neighbors (KNN)
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train, y_train)
knn_pred = knn.predict(X_test)

# Decision Tree
dt = DecisionTreeClassifier()
dt.fit(X_train, y_train)
dt_pred = dt.predict(X_test)

# Random Forest
rf = RandomForestClassifier()
rf.fit(X_train, y_train)
rf_pred = rf.predict(X_test)

# XGBoost
xgb_model = xgb.XGBClassifier()
xgb_model.fit(X_train, y_train)
xgb_pred = xgb_model.predict(X_test)

actual = y_test  # Actual target values
# Membuat confusion matrix dari semua model yang sudah dilakukan
knn_cm = confusion_matrix(actual, knn_pred)
dt_cm = confusion_matrix(actual, dt_pred)
rf_cm = confusion_matrix(actual, rf_pred)
xgb_cm = confusion_matrix(actual, xgb_pred)

# Mengubah confusion matrix ke dataframe
def confusion_matrix_to_dataframe(cm):
    labels = sorted(set(actual))
    df_cm = pd.DataFrame(cm, index=labels, columns=labels)
    df_cm.index.name = 'Actual'
    df_cm.columns.name = 'Predicted'
    return df_cm

# Mengubah confusion matrix ke dataframe dengan label
knn_cm_df = confusion_matrix_to_dataframe(knn_cm)
dt_cm_df = confusion_matrix_to_dataframe(dt_cm)
rf_cm_df = confusion_matrix_to_dataframe(rf_cm)
xgb_cm_df = confusion_matrix_to_dataframe(xgb_cm)

knn_cm_df

dt_cm_df

rf_cm_df

xgb_cm_df

"""### **Mengevaluasi Hasil Pemodelan**"""

# Evaluasi hasil pemodelan menggunakan matriks yang terdapat di klasifikasi
def evaluate(y_true, y_pred):
    cm = confusion_matrix(y_true, y_pred)
    accuracy = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred, average='weighted')
    recall = recall_score(y_true, y_pred, average='weighted')
    f1 = f1_score(y_true, y_pred, average='weighted')
    return cm, accuracy, precision, recall, f1

# Evaluate KNN
knn_cm, knn_accuracy, knn_precision, knn_recall, knn_f1 = evaluate(y_test, knn_pred)

# Evaluate Decision Tree
dt_cm, dt_accuracy, dt_precision, dt_recall, dt_f1 = evaluate(y_test, dt_pred)

# Evaluate Random Forest
rf_cm, rf_accuracy, rf_precision, rf_recall, rf_f1 = evaluate(y_test, rf_pred)

# Evaluate XGBoost
xgb_cm, xgb_accuracy, xgb_precision, xgb_recall, xgb_f1 = evaluate(y_test, xgb_pred)

results_df = pd.DataFrame({
    'Model': ['KNN', 'Decision Tree', 'Random Forest', 'XGBoost'],
    'Accuracy': [knn_accuracy, dt_accuracy, rf_accuracy, xgb_accuracy],
    'Precision': [knn_precision, dt_precision, rf_precision, xgb_precision],
    'F1 Score': [knn_f1, dt_f1, rf_f1, xgb_f1],
    'Recall': [knn_recall, dt_recall, rf_recall, xgb_recall]
})
results_dict = results_df.to_dict()
new_df = pd.DataFrame.from_dict(results_dict)
print(new_df)

"""### **Interpretasi Fitur yang Paling Berpengaruh**"""

rf_feature_importance = rf.feature_importances_
feature_names = X.columns

rf_feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': rf_feature_importance})
rf_feature_importance_df = rf_feature_importance_df.sort_values(by='Importance', ascending=False)
rf_feature_importance_df

"""Variabel yang paling berpengaruh pada dataset Bank Churn Modelling yaitu variabel Age dengan tingkat importance yaitu sebesar 0.285801.

### **Membuat Dataset Baru**
"""

random_values = np.random.randint(2, size=9913)
random_float_values = np.random.rand(9913)
random_values_1 = np.random.randint(1, 11, size=9913)
random_values_2 = np.random.randint(1, 4, size=9913)
random_values_3 = np.random.randint(18, 80, size=9913)
random_values_4 = np.random.randint(18, 80, size=9913)

new_data = pd.DataFrame({
    'CreditScore':random_values_4,
    'Gender':random_values,
    'Age':random_values_3,
    'Tenure':random_values_1,
    'Balance':random_float_values,
    'NumOfProducts':random_values_2,
    'IsActiveMember':random_values,
    'Exited':random_values
})

new_data

"""### **Interpretasi Model ke dalam Data Test**"""

# membuat prediksi dari data test
knn_predictions = knn.predict(X_test)
dt_predictions = dt.predict(X_test)
rf_predictions = rf.predict(X_test)
xgb_predictions = xgb_model.predict(X_test)

# membuat dataframe untuk prediksi
df_knn_predictions = pd.DataFrame({'KNN_Predictions': knn_predictions})
df_dt_predictions = pd.DataFrame({'DT_Predictions': dt_predictions})
df_rf_predictions = pd.DataFrame({'RF_Predictions': rf_predictions})
df_xgb_predictions = pd.DataFrame({'XGB_Predictions': xgb_predictions})

# menggabungkan prediksi dataframe
df_test_combined = pd.concat([X_test.reset_index(drop=True), df_dt_predictions, df_rf_predictions,], axis=1)
df_test_combined

"""Dari hasil pemodelan Decision Tree dan Random Forest yang sudah dilakukan, dapat disimpulkan bahwa:
*   Nasabah bank dengan CreditScore 0.271047 berdasarkan model Decision Tree di prediksi meninggalkan/menutup tabungan, akan tetapi berdasarkan model Random Forest nasabah tersebut di prediksi masih mempertahankan tabungannya di bank.
*   Nasabah bank dengan CreditScore 0.164271, berdasarkan model Decision Tree dan Random Forest di prediksi mempertahankan tabungannya di bank.
*   Nasabah bank dengan CreditScore 0.414784 berdasarkan model Decision Tree dan Random Forest di prediksi masih meninggalkan/menutup tabungannya di bank.



"""